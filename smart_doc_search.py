#!/usr/bin/env python3
"""
æ™ºèƒ½æ–‡æ¡£æœç´¢æ¨¡å—
åŸºäºè‡ªç„¶è¯­è¨€åˆ†æè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨æ–‡æ¡£æœç´¢åŠŸèƒ½
"""

import re
import logging
from typing import List, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class SearchAnalysis:
    """æœç´¢åˆ†æç»“æœ"""
    should_search: bool
    confidence: float  # ç½®ä¿¡åº¦ 0-1
    reason: str
    extracted_query: str

class SmartDocSearchAnalyzer:
    """æ™ºèƒ½æ–‡æ¡£æœç´¢åˆ†æå™¨"""
    
    def __init__(self):
        # æ‰©å±•çš„è§¦å‘å…³é”®è¯
        self.trigger_keywords = [
            # åŸºç¡€å…³é”®è¯
            "æ–‡æ¡£", "çŸ¥è¯†åº“", "wiki", "æŸ¥ä¸€ä¸‹", "æœç´¢", "æ‰¾ä¸€ä¸‹", "å¸®æˆ‘æŸ¥", 
            "èµ„æ–™", "æ•™ç¨‹", "è¯´æ˜", "æ‰‹å†Œ", "æŸ¥æ‰¾", "æŸ¥è¯¢", "æ£€ç´¢", "çœ‹çœ‹", 
            "ç¿»é˜…", "æµè§ˆ", "å‚è€ƒèµ„æ–™", "ç›¸å…³èµ„æ–™", "æ–‡æ¡£èµ„æ–™",
            
            # æ–‡æ¡£ç±»å‹
            "é¡¹ç›®æ–‡æ¡£", "æŠ€æœ¯æ–‡æ¡£", "äº§å“æ–‡æ¡£", "éœ€æ±‚æ–‡æ¡£", "è®¾è®¡æ–‡æ¡£", 
            "å¼€å‘æ–‡æ¡£", "æµ‹è¯•æ–‡æ¡£", "è¿ç»´æ–‡æ¡£", "ç”¨æˆ·æ‰‹å†Œ", "æ“ä½œæ‰‹å†Œ",
            
            # å†…å®¹ç±»å‹
            "æµç¨‹", "è§„èŒƒ", "æ ‡å‡†", "æŒ‡å—", "æœ€ä½³å®è·µ", "åˆ¶åº¦", "è§„å®š"
        ]
        
        # ç–‘é—®è¯
        self.question_indicators = [
            "æ€ä¹ˆ", "å¦‚ä½•", "æ€æ ·", "ä»€ä¹ˆ", "å“ªä¸ª", "å“ªäº›",
            "æœ‰æ²¡æœ‰", "æ˜¯å¦å­˜åœ¨", "èƒ½å¦", "å¯ä»¥", "åº”è¯¥",
            "è¯·æ•™", "è¯·é—®", "æ±‚åŠ©", "å¸®å¿™", "æ±‚"
        ]
        
        # å†…å®¹ç›¸å…³è¯
        self.content_words = [
            "æµç¨‹", "æ­¥éª¤", "æ–¹æ³•", "æ–¹å¼", "æ“ä½œ", "é…ç½®", 
            "è®¾ç½®", "å®‰è£…", "éƒ¨ç½²", "ä½¿ç”¨", "è§„èŒƒ", "æ ‡å‡†", 
            "è¦æ±‚", "è§„å®š", "æ–‡æ¡£", "èµ„æ–™", "ä¿¡æ¯"
        ]
        
        # ä¸Šä¸‹æ–‡æŒ‡ç¤ºè¯
        self.context_indicators = [
            "é¡¹ç›®", "äº§å“", "ç³»ç»Ÿ", "å¹³å°", "å·¥å…·", "æœåŠ¡",
            "sdk", "api", "æ¥å£", "æ¡†æ¶", "ç»„ä»¶", "æ¨¡å—"
        ]
        
        # éœ€æ±‚åŠ¨è¯
        self.need_verbs = [
            "äº†è§£", "ç†Ÿæ‚‰", "æŒæ¡", "å­¦ä¹ ", "ç ”ç©¶", "æŸ¥çœ‹",
            "éœ€è¦", "å‡†å¤‡", "æ•´ç†", "å‚è€ƒ", "æŸ¥é˜…"
        ]
        
        # é—®å€™è¯­ï¼ˆç”¨äºæ’é™¤ï¼‰
        self.greetings = [
            "ä½ å¥½", "æ‚¨å¥½", "hello", "hi", "æ—©ä¸Šå¥½", "ä¸‹åˆå¥½",
            "æ™šä¸Šå¥½", "è¾›è‹¦äº†", "è°¢è°¢", "æ„Ÿè°¢"
        ]
    
    def analyze(self, user_text: str) -> SearchAnalysis:
        """
        åˆ†æç”¨æˆ·æ¶ˆæ¯ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ–‡æ¡£æœç´¢
        
        Args:
            user_text: ç”¨æˆ·æ¶ˆæ¯æ–‡æœ¬
            
        Returns:
            SearchAnalysis: åˆ†æç»“æœ
        """
        text_lower = user_text.lower().strip()
        
        # 1. åŸºç¡€æ£€æŸ¥
        if not text_lower:
            return SearchAnalysis(False, 0.0, "ç©ºæ¶ˆæ¯", "")
        
        # 2. å…³é”®è¯åŒ¹é…ï¼ˆé«˜æƒé‡ï¼‰
        keyword_match = self._check_keywords(text_lower)
        if keyword_match:
            query = self._extract_query(text_lower)
            return SearchAnalysis(True, 0.9, f"åŒ¹é…å…³é”®è¯: {keyword_match}", query)
        
        # 3. ç–‘é—®å¥æ¨¡å¼ï¼ˆä¸­é«˜æƒé‡ï¼‰
        question_match = self._check_question_patterns(text_lower)
        if question_match:
            query = self._extract_query(text_lower)
            return SearchAnalysis(True, 0.8, f"ç–‘é—®å¥æ¨¡å¼: {question_match}", query)
        
        # 4. ä»»åŠ¡å¯¼å‘è¯­å¥ï¼ˆä¸­æƒé‡ï¼‰
        task_match = self._check_task_patterns(text_lower)
        if task_match:
            query = self._extract_query(text_lower)
            return SearchAnalysis(True, 0.7, f"ä»»åŠ¡å¯¼å‘: {task_match}", query)
        
        # 5. ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼ˆä¸­æƒé‡ï¼‰
        context_match = self._check_context_patterns(text_lower)
        if context_match:
            query = self._extract_query(text_lower)
            return SearchAnalysis(True, 0.6, f"ä¸Šä¸‹æ–‡ç›¸å…³: {context_match}", query)
        
        # 6. å¤æ‚æŸ¥è¯¢åˆ¤æ–­ï¼ˆä½æƒé‡ï¼‰
        if self._is_complex_query(text_lower):
            query = self._extract_query(text_lower)
            return SearchAnalysis(True, 0.5, "å¤æ‚æŸ¥è¯¢éœ€è¦æ–‡æ¡£æ”¯æŒ", query)
        
        # 7. ä¸éœ€è¦æœç´¢
        return SearchAnalysis(False, 0.1, "å¸¸è§„å¯¹è¯ï¼Œæ— éœ€æ–‡æ¡£æœç´¢", "")
    
    def _check_keywords(self, text: str) -> str:
        """æ£€æŸ¥å…³é”®è¯åŒ¹é…"""
        for keyword in self.trigger_keywords:
            if keyword.lower() in text:
                return keyword
        return ""
    
    def _check_question_patterns(self, text: str) -> str:
        """æ£€æŸ¥ç–‘é—®å¥æ¨¡å¼"""
        # ç–‘é—®è¯ + å†…å®¹è¯ç»„åˆ
        for indicator in self.question_indicators:
            if indicator in text:
                for content_word in self.content_words:
                    if content_word in text:
                        return f"{indicator}{content_word}"
        
        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        patterns = [
            r"æ€ä¹ˆ.{0,15}(åš|ç”¨|æ“ä½œ|é…ç½®|è®¾ç½®|éƒ¨ç½²)",
            r"å¦‚ä½•.{0,15}(åš|ç”¨|æ“ä½œ|é…ç½®|è®¾ç½®|éƒ¨ç½²)",
            r"(ä»€ä¹ˆæ˜¯|ä»€ä¹ˆæ˜¯|ä»€ä¹ˆå«).{1,20}",
            r".{1,20}(åœ¨å“ª|æ€ä¹ˆæ‰¾|å“ªé‡Œæœ‰)",
            r"(æœ‰æ²¡æœ‰|æ˜¯å¦å­˜åœ¨).{1,20}(æ–‡æ¡£|è¯´æ˜|æ•™ç¨‹|èµ„æ–™)",
            r"(è¯·æ•™|è¯·é—®|æ±‚åŠ©).{1,20}(å¦‚ä½•|æ€ä¹ˆ)"
        ]
        
        for pattern in patterns:
            if re.search(pattern, text):
                return "ç–‘é—®å¥æ¨¡å¼åŒ¹é…"
        
        return ""
    
    def _check_task_patterns(self, text: str) -> str:
        """æ£€æŸ¥ä»»åŠ¡å¯¼å‘è¯­å¥"""
        patterns = [
            r"(éœ€è¦|å‡†å¤‡|æ•´ç†).{0,15}(æ–‡æ¡£|èµ„æ–™|ä¿¡æ¯)",
            r"(äº†è§£|å­¦ä¹ |ç ”ç©¶).{0,15}(æµç¨‹|è§„èŒƒ|æ ‡å‡†|æ“ä½œ)",
            r"(å‚è€ƒ|æŸ¥é˜…).{0,15}(æ–‡æ¡£|èµ„æ–™)",
            r"(æŸ¥æ‰¾|æœç´¢).{0,15}(ç›¸å…³|æœ‰å…³).{0,10}(èµ„æ–™|ä¿¡æ¯)"
        ]
        
        for pattern in patterns:
            if re.search(pattern, text):
                return "ä»»åŠ¡å¯¼å‘è¯­å¥åŒ¹é…"
        
        return ""
    
    def _check_context_patterns(self, text: str) -> str:
        """æ£€æŸ¥ä¸Šä¸‹æ–‡ç›¸å…³æ€§"""
        for indicator in self.context_indicators:
            if indicator in text:
                for verb in self.need_verbs:
                    if verb in text:
                        return f"{verb}{indicator}"
        return ""
    
    def _is_complex_query(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¤æ‚æŸ¥è¯¢"""
        # é•¿åº¦åˆ¤æ–­
        if len(text) < 10:
            return False
            
        # æ’é™¤é—®å€™è¯­
        if any(greeting in text for greeting in self.greetings):
            return False
            
        # ç»“å°¾ä¸æ˜¯ç®€å•æ ‡ç‚¹
        if text.endswith(('?', 'ï¼Ÿ', '.', 'ã€‚', '!', 'ï¼')):
            # ä½†æ˜¯ç–‘é—®å¥å¯èƒ½æ˜¯éœ€è¦æœç´¢çš„
            question_endings = [('?', 'ï¼Ÿ')]
            if any(ending in text for ending in ['?', 'ï¼Ÿ']):
                return True
        
        # åŒ…å«å…·ä½“å†…å®¹è¯æ±‡
        content_indicators = ["æµç¨‹", "æ­¥éª¤", "æ–¹æ³•", "é…ç½®", "ä½¿ç”¨", "æ“ä½œ"]
        if any(indicator in text for indicator in content_indicators):
            return True
            
        return False
    
    def _extract_query(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–æœç´¢å…³é”®è¯"""
        # ç§»é™¤å¸¸è§çš„å‰ç½®è¯
        prefixes = ["å¸®æˆ‘", "è¯·", "æƒ³", "è¦", "æŸ¥æ‰¾", "æœç´¢", "æŸ¥ä¸€ä¸‹", "æ‰¾ä¸€ä¸‹"]
        query_text = text.lower()
        
        for prefix in prefixes:
            if query_text.startswith(prefix):
                query_text = query_text[len(prefix):].strip()
                break
        
        # ç§»é™¤åç¼€è¯
        suffixes = ["çš„æ–‡æ¡£", "çš„èµ„æ–™", "æ€ä¹ˆåš", "å¦‚ä½•åš", "ç›¸å…³ä¿¡æ¯"]
        for suffix in suffixes:
            if query_text.endswith(suffix):
                query_text = query_text[:-len(suffix)].strip()
                break
        
        # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        query_text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', query_text)
        
        return query_text.strip() if query_text else text.strip()

# å…¨å±€å®ä¾‹
_analyzer = SmartDocSearchAnalyzer()

def should_search_documents_smart(user_text: str) -> Tuple[bool, float, str, str]:
    """
    æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦æ–‡æ¡£æœç´¢
    
    Args:
        user_text: ç”¨æˆ·æ¶ˆæ¯æ–‡æœ¬
        
    Returns:
        Tuple[bool, float, str, str]: (æ˜¯å¦æœç´¢, ç½®ä¿¡åº¦, åŸå› , æå–çš„æŸ¥è¯¢è¯)
    """
    analysis = _analyzer.analyze(user_text)
    return analysis.should_search, analysis.confidence, analysis.reason, analysis.extracted_query

def get_search_confidence(user_text: str) -> float:
    """è·å–æœç´¢ç½®ä¿¡åº¦"""
    _, confidence, _, _ = should_search_documents_smart(user_text)
    return confidence

# æµ‹è¯•å‡½æ•°
def test_smart_analyzer():
    """æµ‹è¯•æ™ºèƒ½åˆ†æå™¨"""
    test_cases = [
        "å¸®æˆ‘æŸ¥ä¸€ä¸‹å…¥åº“æµç¨‹çš„æ–‡æ¡£",
        "æ€ä¹ˆé…ç½®è¿™ä¸ªç³»ç»Ÿçš„APIæ¥å£ï¼Ÿ",
        "é¡¹ç›®çš„æŠ€æœ¯è§„èŒƒåœ¨å“ªé‡Œå¯ä»¥æ‰¾åˆ°ï¼Ÿ",
        "æœ‰æ²¡æœ‰å…³äºç”¨æˆ·è®¤è¯çš„è¯´æ˜æ–‡æ¡£ï¼Ÿ",
        "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "è¯·æ•™ä¸€ä¸‹æ•°æ®åº“è®¾è®¡çš„æœ€ä½³å®è·µ",
        "éœ€è¦å‡†å¤‡ä¸€ä»½äº§å“éœ€æ±‚æ–‡æ¡£",
        "äº†è§£å¾®æœåŠ¡æ¶æ„çš„è®¾è®¡æ¨¡å¼",
        "ç®€å•ä»‹ç»ä¸€ä¸‹ä½ ä»¬å…¬å¸",
        "æŸ¥æ‰¾æœ€è¿‘çš„é¡¹ç›®è¿›åº¦æŠ¥å‘Š"
    ]
    
    print("ğŸ¤– æ™ºèƒ½æ–‡æ¡£æœç´¢åˆ†ææµ‹è¯•")
    print("=" * 50)
    
    for text in test_cases:
        should_search, confidence, reason, query = should_search_documents_smart(text)
        status = "ğŸ” éœ€è¦æœç´¢" if should_search else "ğŸ“ æ— éœ€æœç´¢"
        print(f"{status} (ç½®ä¿¡åº¦: {confidence:.1f}) [{reason}]")
        print(f"  è¾“å…¥: {text}")
        if should_search:
            print(f"  æŸ¥è¯¢è¯: {query}")
        print()

if __name__ == "__main__":
    test_smart_analyzer()